{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv into pandas dataframe\n",
    "df = pd.read_csv('withdays.csv')\n",
    "df.loc[(df['year'].isin([2018]))]['category'].unique()\n",
    "df['category'] = df['category'].map({\n",
    "'Graphic Novels & Comics' : 0,\n",
    "'Young Adult Fiction' : 1,\n",
    "'Memoir & Autobiography' : 2,\n",
    "'Picture Books' : 3,\n",
    "'Romance' : 4,\n",
    "'Humor' : 5,\n",
    "'Poetry' : 6,\n",
    "'Horror' : 7,\n",
    "'Young Adult Fantasy' : 8,\n",
    "'Science Fiction' : 9,\n",
    "\"Middle Grade & Children's\" : 10,\n",
    "'History & Biography' : 11,\n",
    "'Nonfiction' : 12,\n",
    "'Fantasy': 13,\n",
    "'Mystery & Thriller' : 14,\n",
    "'Historical Fiction' : 15,\n",
    "'Debut Goodreads Author' : 16,\n",
    "'Fiction' : 17,\n",
    "'Paranormal Fantasy' : 18,\n",
    "'Food & Cookbooks' : 19,\n",
    "'Business Books' : 20,\n",
    "'Science & Technology' : 21,\n",
    "'Goodreads Author' : 22,\n",
    "'Debut Novel' : 23,\n",
    "'Best of the Best' : 24,\n",
    "'Travel & Outdoors' : 25,\n",
    "'Food & Cooking' : 19,\n",
    "'Favorite Book of 2011' : 24,\n",
    "'Debut Author' : 26\n",
    "})\n",
    "df = df.loc[(df['category'].isin([17]))]\n",
    "df = df.replace([np.inf, -np.inf], 32768)\n",
    "df = df.replace([np.nan], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = df[df.duplicated(['name', 'year', 'category'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['num 4 stars', 'num 5 stars', 'average_rating_w', 'num_ratings', 'num_reviews', 'daystocom', 'avgrank_NYTime', 'avgrank_NYTime']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X = df[features]\n",
    "y = df['winner']\n",
    "X_norm = MinMaxScaler().fit_transform(X)\n",
    "chi_selector = SelectKBest(chi2, k=8)\n",
    "chi_selector.fit(X_norm, y)\n",
    "chi_support = chi_selector.get_support()\n",
    "chi_feature = X.loc[:,chi_support].columns.tolist()\n",
    "print(chi_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['num 4 stars', 'num 5 stars', 'average_rating_w', 'num_ratings', 'num_reviews', 'daystocom', 'times_appred_NYTime', 'avgrank_NYTime', 'avgrank_NYTime', 'listnames']\n",
    "data_test = df.loc[(df['year'].isin([2018]))]\n",
    "X_test = data_test[features]\n",
    "y_test = data_test['winner']\n",
    "data_test['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011\n",
      "fitting\n",
      "2012\n",
      "fitting\n",
      "2013\n",
      "fitting\n",
      "2014\n",
      "fitting\n",
      "2015\n",
      "fitting\n",
      "2016\n",
      "fitting\n",
      "2017\n",
      "fitting\n"
     ]
    }
   ],
   "source": [
    "# Use 2011 - 2017 as train data and 2018 as test data\n",
    "logreg = LogisticRegression(solver='liblinear', class_weight = 'balanced')\n",
    "for year in [2011, 2012, 2013, 2014, 2015, 2016, 2017]:\n",
    "    print(year)\n",
    "    data_temp = df.loc[~(df['year'].isin([2018,2019]))]\n",
    "    data_train = data_temp.loc[(data_temp['year'].isin([year]))]\n",
    "    X_train = data_train[features]\n",
    "    y_train = data_train['winner']\n",
    "    if(len(X_train) != 0) : \n",
    "        print('fitting')\n",
    "        logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    16\n",
       "1     4\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = logreg.predict(X_test)\n",
    "predictions = pd.DataFrame(y_pred)\n",
    "predictions[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.79      0.86        19\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.75        20\n",
      "   macro avg       0.47      0.39      0.43        20\n",
      "weighted avg       0.89      0.75      0.81        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 4 1 0\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test,y_pred).ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.75\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', metrics.accuracy_score(y_test, y_pred))\n",
    "#print('F1 score:', metrics.f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rfc = RandomForestClassifier(n_estimators=10, class_weight = 'balanced')\n",
    "for year in [2011, 2012, 2013, 2014, 2015, 2016, 2017]:\n",
    "    data_temp = df.loc[~(df['year'].isin([2018,2019]))]\n",
    "    data_train = data_temp.loc[(data_temp['year'].isin([year]))]\n",
    "    X_train = data_train[features]\n",
    "    y_train = data_train['winner']\n",
    "    if(len(X_train) != 0) : \n",
    "        rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        19\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.47      0.50      0.49        20\n",
      "weighted avg       0.90      0.95      0.93        20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/negar/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "rfc_pred = rfc.predict(X_test)\n",
    "metrics.accuracy_score(y_test, rfc_pred)\n",
    "print(classification_report(y_test, rfc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 0 1 0\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, rfc_pred).ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "for year in [2011, 2012, 2013, 2014, 2015, 2016, 2017]:\n",
    "    data_temp = df.loc[~(df['year'].isin([2018,2019]))]\n",
    "    data_train = data_temp.loc[(data_temp['year'].isin([year]))]\n",
    "    X_train = data_train[features]\n",
    "    y_train = data_train['winner']\n",
    "    if(len(X_train) != 0) : \n",
    "        gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    20\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = gnb.predict(X_test)\n",
    "predictions = pd.DataFrame(y_pred)\n",
    "predictions[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        19\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.47      0.50      0.49        20\n",
      "weighted avg       0.90      0.95      0.93        20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/negar/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 0 1 0\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     nominee       0.94      0.79      0.86        19\n",
      "      winner       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.75        20\n",
      "   macro avg       0.47      0.39      0.43        20\n",
      "weighted avg       0.89      0.75      0.81        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_GB = GradientBoostingClassifier(n_estimators=1000)\n",
    "for year in [2011, 2012, 2013, 2014, 2015, 2016, 2017]:\n",
    "    data_temp = df.loc[~(df['year'].isin([2018,2019]))]\n",
    "    data_train = data_temp.loc[(data_temp['year'].isin([year]))]\n",
    "    X_train = data_train[features]\n",
    "    y_train = data_train['winner']\n",
    "    if(len(X_train) != 0) : \n",
    "        model_GB.fit(X_train , y_train)\n",
    "y_pred = model_GB.predict(X_test)\n",
    "target_names = ['nominee', 'winner']\n",
    "print(classification_report(y_test, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 4 1 0\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     nominee       0.94      0.79      0.86        19\n",
      "      winner       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.75        20\n",
      "   macro avg       0.47      0.39      0.43        20\n",
      "weighted avg       0.89      0.75      0.81        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost classifier builds a strong classifier by combining multiple\n",
    "# poorly performing classifiers to get high accuracy strong classifier.\n",
    "model_ad = AdaBoostClassifier()\n",
    "for year in [2011, 2012, 2013, 2014, 2015, 2016, 2017]:\n",
    "    data_temp = df.loc[~(df['year'].isin([2018,2019]))]\n",
    "    data_train = data_temp.loc[(data_temp['year'].isin([year]))]\n",
    "    X_train = data_train[features]\n",
    "    y_train = data_train['winner']\n",
    "    if(len(X_train) != 0) : \n",
    "        model_ad.fit(X_train , y_train)\n",
    "        \n",
    "y_pred = model_ad.predict(X_test)\n",
    "target_names = ['nominee', 'winner']\n",
    "print(classification_report(y_test, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 4 1 0\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     nominee       0.95      1.00      0.97        19\n",
      "      winner       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.47      0.50      0.49        20\n",
      "weighted avg       0.90      0.95      0.93        20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/negar/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "for year in [2011, 2012, 2013, 2014, 2015, 2016, 2017]:\n",
    "    data_temp = df.loc[~(df['year'].isin([2018,2019]))]\n",
    "    data_train = data_temp.loc[(data_temp['year'].isin([year]))]\n",
    "    X_train = data_train[features]\n",
    "    y_train = data_train['winner']\n",
    "    if(len(X_train) != 0) : \n",
    "        knn.fit(X_train , y_train)\n",
    "        \n",
    "y_pred = knn.predict(X_test)\n",
    "target_names = ['nominee', 'winner']\n",
    "print(classification_report(y_test, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 0 1 0\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     nominee       0.95      1.00      0.97        19\n",
      "      winner       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.47      0.50      0.49        20\n",
      "weighted avg       0.90      0.95      0.93        20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/negar/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "for year in [2011, 2012, 2013, 2014, 2015, 2016, 2017]:\n",
    "    data_temp = df.loc[~(df['year'].isin([2018,2019]))]\n",
    "    data_train = data_temp.loc[(data_temp['year'].isin([year]))]\n",
    "    X_train = data_train[features]\n",
    "    y_train = data_train['winner']\n",
    "    if(len(X_train) != 0) : \n",
    "        clf.fit(X_train , y_train)\n",
    "        \n",
    "y_pred = clf.predict(X_test)\n",
    "target_names = ['nominee', 'winner']\n",
    "print(classification_report(y_test, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
