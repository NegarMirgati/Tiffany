{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>writer</th>\n",
       "      <th>category</th>\n",
       "      <th>winner</th>\n",
       "      <th>num 1 stars</th>\n",
       "      <th>num 2 stars</th>\n",
       "      <th>num 3 stars</th>\n",
       "      <th>num 4 stars</th>\n",
       "      <th>num 5 stars</th>\n",
       "      <th>average rating</th>\n",
       "      <th>average_rating_w</th>\n",
       "      <th>num_ratings</th>\n",
       "      <th>num_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2013</td>\n",
       "      <td>The House Girl</td>\n",
       "      <td>['Tara Conklin']</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010033</td>\n",
       "      <td>0.033445</td>\n",
       "      <td>0.290970</td>\n",
       "      <td>0.464883</td>\n",
       "      <td>0.200669</td>\n",
       "      <td>3.812709</td>\n",
       "      <td>3.76</td>\n",
       "      <td>75029</td>\n",
       "      <td>7891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>341</td>\n",
       "      <td>2013</td>\n",
       "      <td>Orphan Train</td>\n",
       "      <td>['Christina Baker Kline']</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006689</td>\n",
       "      <td>0.023411</td>\n",
       "      <td>0.137124</td>\n",
       "      <td>0.508361</td>\n",
       "      <td>0.324415</td>\n",
       "      <td>4.120401</td>\n",
       "      <td>4.09</td>\n",
       "      <td>417138</td>\n",
       "      <td>35762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342</td>\n",
       "      <td>2013</td>\n",
       "      <td>The Storyteller</td>\n",
       "      <td>['Jodi Picoult']</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.023490</td>\n",
       "      <td>0.097315</td>\n",
       "      <td>0.382550</td>\n",
       "      <td>0.489933</td>\n",
       "      <td>4.325503</td>\n",
       "      <td>3.97</td>\n",
       "      <td>3482177</td>\n",
       "      <td>209189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>343</td>\n",
       "      <td>2013</td>\n",
       "      <td>The Rosie Project</td>\n",
       "      <td>['Graeme Simsion']</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003367</td>\n",
       "      <td>0.043771</td>\n",
       "      <td>0.127946</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>4.144781</td>\n",
       "      <td>3.92</td>\n",
       "      <td>525116</td>\n",
       "      <td>51377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>2013</td>\n",
       "      <td>And the Mountains Echoed</td>\n",
       "      <td>['Khaled Hosseini']</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.373737</td>\n",
       "      <td>0.326599</td>\n",
       "      <td>3.976431</td>\n",
       "      <td>4.29</td>\n",
       "      <td>3615182</td>\n",
       "      <td>152304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3242</td>\n",
       "      <td>2016</td>\n",
       "      <td>Shelter</td>\n",
       "      <td>['Jung Yun']</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014599</td>\n",
       "      <td>0.076642</td>\n",
       "      <td>0.193431</td>\n",
       "      <td>0.474453</td>\n",
       "      <td>0.240876</td>\n",
       "      <td>3.850365</td>\n",
       "      <td>3.73</td>\n",
       "      <td>6795</td>\n",
       "      <td>1146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3243</td>\n",
       "      <td>2016</td>\n",
       "      <td>Silver Threads</td>\n",
       "      <td>['Bette Lee Crosby']</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.206349</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>4.365079</td>\n",
       "      <td>4.23</td>\n",
       "      <td>16847</td>\n",
       "      <td>4058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3244</td>\n",
       "      <td>2016</td>\n",
       "      <td>Another Brooklyn</td>\n",
       "      <td>['Jacqueline Woodson']</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006689</td>\n",
       "      <td>0.036789</td>\n",
       "      <td>0.140468</td>\n",
       "      <td>0.458194</td>\n",
       "      <td>0.357860</td>\n",
       "      <td>4.123746</td>\n",
       "      <td>4.04</td>\n",
       "      <td>181528</td>\n",
       "      <td>28868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3245</td>\n",
       "      <td>2016</td>\n",
       "      <td>Everybody's Fool</td>\n",
       "      <td>['Richard Russo']</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.021341</td>\n",
       "      <td>0.146341</td>\n",
       "      <td>0.432927</td>\n",
       "      <td>0.387195</td>\n",
       "      <td>4.161585</td>\n",
       "      <td>3.90</td>\n",
       "      <td>237228</td>\n",
       "      <td>19535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3246</td>\n",
       "      <td>2016</td>\n",
       "      <td>Nutshell</td>\n",
       "      <td>['Ian McEwan']</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060317</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.174603</td>\n",
       "      <td>0.396825</td>\n",
       "      <td>0.320635</td>\n",
       "      <td>3.869841</td>\n",
       "      <td>3.72</td>\n",
       "      <td>891698</td>\n",
       "      <td>60431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      year                      name                     writer  category  \\\n",
       "340   2013            The House Girl           ['Tara Conklin']        17   \n",
       "341   2013              Orphan Train  ['Christina Baker Kline']        17   \n",
       "342   2013           The Storyteller           ['Jodi Picoult']        17   \n",
       "343   2013         The Rosie Project         ['Graeme Simsion']        17   \n",
       "344   2013  And the Mountains Echoed        ['Khaled Hosseini']        17   \n",
       "...    ...                       ...                        ...       ...   \n",
       "3242  2016                   Shelter               ['Jung Yun']        17   \n",
       "3243  2016            Silver Threads       ['Bette Lee Crosby']        17   \n",
       "3244  2016          Another Brooklyn     ['Jacqueline Woodson']        17   \n",
       "3245  2016          Everybody's Fool          ['Richard Russo']        17   \n",
       "3246  2016                  Nutshell             ['Ian McEwan']        17   \n",
       "\n",
       "      winner  num 1 stars  num 2 stars  num 3 stars  num 4 stars  num 5 stars  \\\n",
       "340        0     0.010033     0.033445     0.290970     0.464883     0.200669   \n",
       "341        0     0.006689     0.023411     0.137124     0.508361     0.324415   \n",
       "342        0     0.006711     0.023490     0.097315     0.382550     0.489933   \n",
       "343        0     0.003367     0.043771     0.127946     0.454545     0.370370   \n",
       "344        1     0.010101     0.030303     0.259259     0.373737     0.326599   \n",
       "...      ...          ...          ...          ...          ...          ...   \n",
       "3242       0     0.014599     0.076642     0.193431     0.474453     0.240876   \n",
       "3243       0     0.015873     0.047619     0.111111     0.206349     0.619048   \n",
       "3244       0     0.006689     0.036789     0.140468     0.458194     0.357860   \n",
       "3245       0     0.012195     0.021341     0.146341     0.432927     0.387195   \n",
       "3246       0     0.060317     0.047619     0.174603     0.396825     0.320635   \n",
       "\n",
       "      average rating  average_rating_w  num_ratings  num_reviews  \n",
       "340         3.812709              3.76        75029         7891  \n",
       "341         4.120401              4.09       417138        35762  \n",
       "342         4.325503              3.97      3482177       209189  \n",
       "343         4.144781              3.92       525116        51377  \n",
       "344         3.976431              4.29      3615182       152304  \n",
       "...              ...               ...          ...          ...  \n",
       "3242        3.850365              3.73         6795         1146  \n",
       "3243        4.365079              4.23        16847         4058  \n",
       "3244        4.123746              4.04       181528        28868  \n",
       "3245        4.161585              3.90       237228        19535  \n",
       "3246        3.869841              3.72       891698        60431  \n",
       "\n",
       "[160 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv into pandas dataframe\n",
    "df = pd.read_csv('goodreads_output.csv')\n",
    "df.loc[(df['year'].isin([2018]))]['category'].unique()\n",
    "df['category'] = df['category'].map({\n",
    "'Graphic Novels & Comics' : 0,\n",
    "'Young Adult Fiction' : 1,\n",
    "'Memoir & Autobiography' : 2,\n",
    "'Picture Books' : 3,\n",
    "'Romance' : 4,\n",
    "'Humor' : 5,\n",
    "'Poetry' : 6,\n",
    "'Horror' : 7,\n",
    "'Young Adult Fantasy' : 8,\n",
    "'Science Fiction' : 9,\n",
    "\"Middle Grade & Children's\" : 10,\n",
    "'History & Biography' : 11,\n",
    "'Nonfiction' : 12,\n",
    "'Fantasy': 13,\n",
    "'Mystery & Thriller' : 14,\n",
    "'Historical Fiction' : 15,\n",
    "'Debut Goodreads Author' : 16,\n",
    "'Fiction' : 17,\n",
    "'Paranormal Fantasy' : 18,\n",
    "'Food & Cookbooks' : 19,\n",
    "'Business Books' : 20,\n",
    "'Science & Technology' : 21,\n",
    "'Goodreads Author' : 22,\n",
    "'Debut Novel' : 23,\n",
    "'Best of the Best' : 24,\n",
    "'Travel & Outdoors' : 25,\n",
    "'Food & Cooking' : 19,\n",
    "'Favorite Book of 2011' : 24,\n",
    "'Debut Author' : 26\n",
    "})\n",
    "df = df.loc[(df['category'].isin([17]))]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = df[df.duplicated(['name', 'year', 'category'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['average_rating_w', 'num_ratings', 'num_reviews']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X = df[features]\n",
    "y = df['winner']\n",
    "X_norm = MinMaxScaler().fit_transform(X)\n",
    "chi_selector = SelectKBest(chi2, k=3)\n",
    "chi_selector.fit(X_norm, y)\n",
    "chi_support = chi_selector.get_support()\n",
    "chi_feature = X.loc[:,chi_support].columns.tolist()\n",
    "print(chi_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['num 4 stars', 'num 5 stars', 'average_rating_w', 'num_ratings', 'num_reviews']\n",
    "data_test = df.loc[(df['year'].isin([2018]))]\n",
    "X_test = data_test[features]\n",
    "y_test = data_test['winner']\n",
    "data_test['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011\n",
      "fitting\n",
      "2012\n",
      "fitting\n",
      "2013\n",
      "fitting\n",
      "2014\n",
      "fitting\n",
      "2015\n",
      "fitting\n",
      "2016\n",
      "fitting\n",
      "2017\n",
      "fitting\n"
     ]
    }
   ],
   "source": [
    "# Use 2011 - 2017 as train data and 2018 as test data\n",
    "logreg = LogisticRegression(solver='liblinear', class_weight = 'balanced')\n",
    "for year in [2011, 2012, 2013, 2014, 2015, 2016, 2017]:\n",
    "    print(year)\n",
    "    data_temp = df.loc[~(df['year'].isin([2018,2019]))]\n",
    "    data_train = data_temp.loc[(data_temp['year'].isin([year]))]\n",
    "    X_train = data_train[features]\n",
    "    y_train = data_train['winner']\n",
    "    if(len(X_train) != 0) : \n",
    "        print('fitting')\n",
    "        logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    13\n",
       "1     7\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = logreg.predict(X_test)\n",
    "predictions = pd.DataFrame(y_pred)\n",
    "predictions[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.68      0.81        19\n",
      "          1       0.14      1.00      0.25         1\n",
      "\n",
      "avg / total       0.96      0.70      0.78        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 6 0 1\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test,y_pred).ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', metrics.accuracy_score(y_test, y_pred))\n",
    "#print('F1 score:', metrics.f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rfc = RandomForestClassifier(n_estimators=10, class_weight = 'balanced')\n",
    "for year in [2011, 2012, 2013, 2014, 2015, 2016, 2017]:\n",
    "    data_temp = df.loc[~(df['year'].isin([2018,2019]))]\n",
    "    data_train = data_temp.loc[(data_temp['year'].isin([year]))]\n",
    "    X_train = data_train[features]\n",
    "    y_train = data_train['winner']\n",
    "    if(len(X_train) != 0) : \n",
    "        rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97        19\n",
      "          1       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.90      0.95      0.93        20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/negar/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "rfc_pred = rfc.predict(X_test)\n",
    "metrics.accuracy_score(y_test, rfc_pred)\n",
    "print(classification_report(y_test, rfc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 0 1 0\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, rfc_pred).ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "for year in [2011, 2012, 2013, 2014, 2015, 2016, 2017]:\n",
    "    data_temp = df.loc[~(df['year'].isin([2018,2019]))]\n",
    "    data_train = data_temp.loc[(data_temp['year'].isin([year]))]\n",
    "    X_train = data_train[features]\n",
    "    y_train = data_train['winner']\n",
    "    if(len(X_train) != 0) : \n",
    "        gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    20\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = gnb.predict(X_test)\n",
    "predictions = pd.DataFrame(y_pred)\n",
    "predictions[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97        19\n",
      "          1       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.90      0.95      0.93        20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/negar/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 0 1 0\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    nominee       0.95      1.00      0.97        19\n",
      "     winner       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.90      0.95      0.93        20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/negar/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "model_GB = GradientBoostingClassifier(n_estimators=1000)\n",
    "for year in [2011, 2012, 2013, 2014, 2015, 2016, 2017]:\n",
    "    data_temp = df.loc[~(df['year'].isin([2018,2019]))]\n",
    "    data_train = data_temp.loc[(data_temp['year'].isin([year]))]\n",
    "    X_train = data_train[features]\n",
    "    y_train = data_train['winner']\n",
    "    if(len(X_train) != 0) : \n",
    "        model_GB.fit(X_train , y_train)\n",
    "y_pred = model_GB.predict(X_test)\n",
    "target_names = ['nominee', 'winner']\n",
    "print(classification_report(y_test, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 0 1 0\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    nominee       0.95      1.00      0.97        19\n",
      "     winner       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.90      0.95      0.93        20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/negar/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost classifier builds a strong classifier by combining multiple\n",
    "# poorly performing classifiers to get high accuracy strong classifier.\n",
    "model_ad = AdaBoostClassifier()\n",
    "for year in [2011, 2012, 2013, 2014, 2015, 2016, 2017]:\n",
    "    data_temp = df.loc[~(df['year'].isin([2018,2019]))]\n",
    "    data_train = data_temp.loc[(data_temp['year'].isin([year]))]\n",
    "    X_train = data_train[features]\n",
    "    y_train = data_train['winner']\n",
    "    if(len(X_train) != 0) : \n",
    "        model_ad.fit(X_train , y_train)\n",
    "        \n",
    "y_pred = model_ad.predict(X_test)\n",
    "target_names = ['nominee', 'winner']\n",
    "print(classification_report(y_test, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 0 1 0\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    nominee       0.95      1.00      0.97        19\n",
      "     winner       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.90      0.95      0.93        20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/negar/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "for year in [2011, 2012, 2013, 2014, 2015, 2016, 2017]:\n",
    "    data_temp = df.loc[~(df['year'].isin([2018,2019]))]\n",
    "    data_train = data_temp.loc[(data_temp['year'].isin([year]))]\n",
    "    X_train = data_train[features]\n",
    "    y_train = data_train['winner']\n",
    "    if(len(X_train) != 0) : \n",
    "        knn.fit(X_train , y_train)\n",
    "        \n",
    "y_pred = knn.predict(X_test)\n",
    "target_names = ['nominee', 'winner']\n",
    "print(classification_report(y_test, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 0 1 0\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    nominee       0.95      1.00      0.97        19\n",
      "     winner       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.90      0.95      0.93        20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/negar/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "for year in [2011, 2012, 2013, 2014, 2015, 2016, 2017]:\n",
    "    data_temp = df.loc[~(df['year'].isin([2018,2019]))]\n",
    "    data_train = data_temp.loc[(data_temp['year'].isin([year]))]\n",
    "    X_train = data_train[features]\n",
    "    y_train = data_train['winner']\n",
    "    if(len(X_train) != 0) : \n",
    "        clf.fit(X_train , y_train)\n",
    "        \n",
    "y_pred = clf.predict(X_test)\n",
    "target_names = ['nominee', 'winner']\n",
    "print(classification_report(y_test, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
