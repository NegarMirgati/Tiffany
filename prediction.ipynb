{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 26, 23, 24, 25])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv into pandas dataframe\n",
    "df = pd.read_csv('goodreads_output.csv')\n",
    "df['category'] = df['category'].map({\n",
    "'Graphic Novels & Comics' : 0,\n",
    "'Young Adult Fiction' : 1,\n",
    "'Memoir & Autobiography' : 2,\n",
    "'Picture Books' : 3,\n",
    "'Romance' : 4,\n",
    "'Humor' : 5,\n",
    "'Poetry' : 6,\n",
    "'Horror' : 7,\n",
    "'Young Adult Fantasy' : 8,\n",
    "'Science Fiction' : 9,\n",
    "\"Middle Grade & Children's\" : 10,\n",
    "'History & Biography' : 11,\n",
    "'Nonfiction' : 12,\n",
    "'Fantasy': 13,\n",
    "'Mystery & Thriller' : 14,\n",
    "'Historical Fiction' : 15,\n",
    "'Debut Goodreads Author' : 16,\n",
    "'Fiction' : 17,\n",
    "'Paranormal Fantasy' : 18,\n",
    "'Food & Cookbooks' : 19,\n",
    "'Business Books' : 20,\n",
    "'Science & Technology' : 21,\n",
    "'Goodreads Author' : 22,\n",
    "'Debut Novel' : 23,\n",
    "'Best of the Best' : 24,\n",
    "'Travel & Outdoors' : 25,\n",
    "'Food & Cooking' : 19,\n",
    "'Favorite Book of 2011' : 24,\n",
    "'Debut Author' : 26\n",
    "})\n",
    "df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = df[df.duplicated(['name', 'year', 'category'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_test = df.loc[(df['year'].isin([2018]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n"
     ]
    }
   ],
   "source": [
    "# first method : use cathegory as a feature\n",
    "# Use 2011 - 2017 as train data and 2018 as test data\n",
    "logreg = LogisticRegression(solver='liblinear', class_weight = 'balanced')\n",
    "\n",
    "for year in [2011, 2012, 2013, 2014, 2015, 2016, 2017]:\n",
    "    print(year)\n",
    "    data_temp = df.loc[~(df['year'].isin([2018,2019]))]\n",
    "    data_train = data_temp.loc[(data_temp['year'].isin([year]))]\n",
    "    X_train = data_train[[ 'category', 'average rating', 'num 1 stars', 'num 2 stars', 'num 3 stars', 'num 4 stars', 'num 5 stars']]\n",
    "    y_train = data_train['winner']\n",
    "    logreg.fit(X_train, y_train)\n",
    "    \n",
    "X_test = data_test[['num 1 stars', 'num 2 stars', 'num 3 stars', 'num 4 stars', 'num 5 stars', 'category', 'average rating']]\n",
    "y_test = data_test['winner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    365\n",
       "1     59\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = logreg.predict(X_test)\n",
    "predictions = pd.DataFrame(y_pred)\n",
    "predictions[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.86      0.90       403\n",
      "          1       0.05      0.14      0.08        21\n",
      "\n",
      "avg / total       0.91      0.83      0.86       424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7995283018867925\n",
      "F1 score: 0.8470132656406677\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', metrics.accuracy_score(y_test, y_pred))\n",
    "print('F1 score:', metrics.f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_new =  pd.DataFrame(y_pred)\n",
    "#y_test['preds'] = y_pred\n",
    "#df_out = pd.merge(df,y_test[['preds']], how = 'left',left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97       403\n",
      "          1       0.00      0.00      0.00        21\n",
      "\n",
      "avg / total       0.90      0.95      0.93       424\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/negar/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# first method : use cathegory as a feature\n",
    "# Use 2011 - 2017 as train data and 2018 as test data\n",
    "rfc = RandomForestClassifier(n_estimators=10, class_weight = 'balanced')\n",
    "\n",
    "for year in [2011, 2012, 2013, 2014, 2015, 2016, 2017]:\n",
    "    print(year)\n",
    "    data_temp = df.loc[~(df['year'].isin([2018,2019]))]\n",
    "    data_train = data_temp.loc[(data_temp['year'].isin([year]))]\n",
    "    X_train = data_train[[ 'category', 'average rating', 'num 1 stars', 'num 2 stars', 'num 3 stars', 'num 4 stars', 'num 5 stars']]\n",
    "    y_train = data_train['winner']\n",
    "    rfc.fit(X_train, y_train)\n",
    "    \n",
    "X_test = data_test[['num 1 stars', 'num 2 stars', 'num 3 stars', 'num 4 stars', 'num 5 stars', 'category', 'average rating']]\n",
    "y_test = data_test['winner']\n",
    "rfc_pred = rfc.predict(X_test)\n",
    "metrics.accuracy_score(y_test, rfc_pred)\n",
    "print(classification_report(y_test, rfc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/negar/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
